{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "Y_temp = data[:,0]\n",
    "Y = Y_temp.reshape(-1)\n",
    "Y = np.eye(10)[Y]\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Y_temp = data[:,0]\n",
    "\n",
    "Y = np.zeros((X.shape[0], 10))\n",
    "for i in Y_temp:\n",
    "    for j in range(10):\n",
    "        if Y_temp[i]==j:\n",
    "            Y[i][j]=1\n",
    "        \n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[ : , 1:]\n",
    "\n",
    "X = np.reshape(X, (Y.shape[0], 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4f55e44c50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[8], cmap = 'gray', vmin=0, vmax = 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with predetermined number of layers\n",
    "\n",
    "We start with a three layers network for digit recognition. Let's train the weigths through backpropagation for this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_initialisation():\n",
    "    \n",
    "    \"\"\"Function supposed to initialize with random numbers the weights and bias \n",
    "    for a neural network of 3 layers with respectively 16, 16 and 10 neurons\n",
    "    Our input layer is of size 784 (28x28 pixel image)\"\"\"\n",
    "    \n",
    "    W1 = np.random.randn(16, 784)\n",
    "    b1 = np.zeros((16, 1))\n",
    "    \n",
    "    W2 = np.random.randn(16, 16)\n",
    "    b2 = np.zeros((16, 1))\n",
    "    \n",
    "    W3 = np.random.randn(10, 16)\n",
    "    b3 = np.zeros((10, 1))\n",
    "\n",
    "    \n",
    "    parameter ={\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2,\n",
    "        \"W3\": W3,\n",
    "        \"b3\": b3\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1 + npexp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(W, A, b):\n",
    "    Z = np.dot(W, A)+b\n",
    "    \n",
    "    cache  = (W, A, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(W, A_prev, b)\n",
    "        A , activation_cache= sigmoid(Z)\n",
    "    \n",
    "    else if activation == \"relu\n",
    "        Z, linear_cache = linear_forward(W, A_prev, b)\n",
    "        A , activation_cache= relu(Z)\n",
    "     \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "        \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, Y):\n",
    "    \n",
    "    cost = (-1/m)* np.sum(Y.dot(np.log(A)) + (1-Y).dot(np.log(1 - A)))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape==())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1/m)*np.dot(dZ, A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, activation_cache):\n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "     \n",
    "    dZ[Z <= 0] = 0 #which is equal to 0 when Z <= 0\n",
    "    \n",
    "    assert(dZ.shape==Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, activation_cache):\n",
    "    Z = activation_cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    \n",
    "    s = sigmoid(Z)\n",
    "    dZ = dA * s * (1-s)  #sigmoid derivative\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters)\n",
    "    \n",
    "    for i in range L:\n",
    "        parameters[\"W\"+ str(i+1)]=parameters[\"W\"+str(i+1)]-learning_rate*grads[\"dW\"+str(i+1)]\n",
    "        parameters[\"b\"+ str(i+1)]=parameters[\"b\"+str(i+1)]-learning_rate*grads[\"db\"+str(i+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_layers_network(X, Y, learning_rate = 0.0075, num_iterations = 3000, ):\n",
    "    \n",
    "    grads = {}\n",
    "    costs = []\n",
    "    L = len(caches) #the number of layers\n",
    "    m = X.shape[1]\n",
    "    A0 = X\n",
    "    \n",
    "    \n",
    "    #initializing\n",
    "    parameters = random_initialisation()\n",
    "    \n",
    "    \n",
    "    for n in range num_iterations: \n",
    "        #forwardProp\n",
    "        Z1, _ = linear_forward(parameters[\"W1\"], A0, parameters[\"b1\"])\n",
    "        A1, cache1 = activation_function(Z1, \"relu\") \n",
    "\n",
    "        Z2, _ = linear_forward(parameters[\"W2\"], A1, parameters[\"b2\"])\n",
    "        A2, cache2 = activation_function(Z2, \"relu\") \n",
    "\n",
    "        Z3, _ = linear_forward(parameters[\"W3\"], A2, parameters[\"b3\"])\n",
    "        A3, cache3 = activation_function(Z3, \"sigmoid\") \n",
    "        \n",
    "        \n",
    "        #compute cost\n",
    "        cost = compute_cost(A3, Y)\n",
    "        \n",
    "        #backpropagation\n",
    "        dA3 = - (np.divide(Y, A3) - np.divide(1-Y, 1-A3))\n",
    "\n",
    "        dA2, dW3, db3 = linear_activation_backward(dA3, cache3, \"sigmoid\")\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"relu\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
    "\n",
    "        grads['dW1']= dW1\n",
    "        grads['db1']= db1\n",
    "        grads['dW2']= dW2\n",
    "        grads['db2']= db2\n",
    "        grads['dW3']= dW3\n",
    "        grads['db3']= db3\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "    \n",
    "        #print cost every 100 iterations\n",
    "        if n%100==0:\n",
    "            print(\"Cost after iteration {} : {}\".format(n, np.squeeze(cost)))\n",
    "            costs.append(cost)\n",
    "    \n",
    "    #let's plot cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('num iterations')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
